%!TEX root = ../report.tex
\chapter{Evaluation and Discussion}
\label{cha:Discussion}

\section{Discussion}
\label{sec:Discussion}
When traversing an rdf graph, the predicates chosen will have a great effect on the time used. Knowing the data searching through will help when choosing the predicate to follow. When choosing what predicates to use in the search, the goal should be to minimize the amount of unnecessary nodes found and followed. When removing predicates, the obvious downside is the possibility of missing some results, and decreasing the accuracy.

BFS search can take ua a lot of memory. This is because all nodes have to be stored while searching. It is possible to limit the maximum distance a node can be from the root node, which in turn will limit the amount of nodes visited, and reduce the memory needed. When testing without any form of pruning, the computer would run out of memory. Because of this, no results are gathered from a method following all edges of each node. Following all edges on each node would also lead to highly connected category nodes being discovered. With more than 120 million nodes, and 350.000 classes, each node would on average be connected to more than 340 other nodes, from the classes alone. Following this with a depth of 3, the average search would visit more than 40 million nodes.

From the results we can see that reducing the amount of nodes traversed results in significantly less time used for a query. By incorporating some natural language processing methods, and inferring structure from the users keywords, similar to those seen in \cite{4812421, aqualog}, the amount of nodes can be reduced. Using natural language processing to infer a category would make it possible to select a small set of predicates to follow. Such a category could also be used with the taxonomy of YAGO to only select nodes that fits.

Reduction of root nodes is the most important for increasing speed. To be able to reduce the roots while maintaining accuracy, using more sophisticated natural language processing is an option. Implementing a system for inferring structure from the keywords would make it possible to find connections where the object and predicate is related to the query. Using the taxonomy from YAGO, this is possible to implement.

Natural language processing also have the possible benefit of pruning some predicates. This would reduce the amount of vertices visited, and if this can be done without reducing accuracy, it would greatly increase the effctivness when searching through RDF graphs.

Using an index for keywords can be used to find shortest paths without exploring the entire graph. Such a shortest path should be the best result for a given keyword, but depending on how it is implemented, it might not find the best solution for vertices containing multiple keywords. This is because a vertex containing more than one keyword at a higher depth may score better than multiple vertices containing one keyword each at a lower depth. The score would depend on the amount of keywords in a single vertex, and the difference in depth. If even a single vertex is found at the same depth as the multi-keyword vertex, the result tree with the fewest vertices will score better. 


\subsection{limitations}
- choice of query terms\\
- Root nodes\\

Some temporal vertices in YAGO use a so called ``wildcard date''. These dates contain a ``\#'' symbol for parts of the date, making it impossible to directly query such dates when selecting roots. This means that the temporal search have some possible vertices missing from the set of root nodes used when traversing the graph. This could have been remedied by creating two new temporal vertices for such dates, one start date indicating the lowest possible value a wildcard date could have, and an end date indicating the highest possible value a wildcard could have.

When searching spatiotemporal data the wildcards have been used. This was done by adding some logic comparing the wildcards found connected to spatial vertices to the date range in the search. By doing this, the spatiotemporal search should be more accurate, and should retrieve some nodes that a regular temporal search would not retrieve.

